import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
import logging
import re

class WardaScraper:
    def __init__(self):
        self.session = requests.Session()
        self.setup_headers()
        self.data = []
        
    def setup_headers(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ar-SA,ar;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(self.headers)
    
    def scrape_aqar(self, city, property_type):
        """Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Aqar.fm"""
        try:
            city_map = {
                'Ø§Ù„Ø±ÙŠØ§Ø¶': 'riyadh', 
                'Ø¬Ø¯Ø©': 'jeddah', 
                'Ø§Ù„Ø¯Ù…Ø§Ù…': 'dammam', 
                'Ù…ÙƒØ©': 'makkah', 
                'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©': 'almadinah'
            }
            prop_map = {'Ø´Ù‚Ø©': 'apartment', 'ÙÙŠÙ„Ø§': 'villa', 'Ø£Ø±Ø¶': 'land'}
            
            search_city = city_map.get(city, city)
            search_prop = prop_map.get(property_type, property_type)
            
            url = f"https://aqar.fm/search?city={search_city}&type={search_prop}"
            print(f"ğŸ” Ø¬Ø§Ø±ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Aqar: {city} - {property_type}")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¨Ù…Ø±ÙˆÙ†Ø© ÙÙŠ Ø§Ù„Ø¹Ù†Ø§ØµØ±
            listings = soup.find_all(['div', 'article'], class_=lambda x: x and any(
                cls in str(x).lower() for cls in ['card', 'listing', 'property', 'item']
            ))
            
            for listing in listings[:20]:
                property_data = self.parse_aqar_listing(listing, city, property_type)
                if property_data and self.validate_property_data(property_data):
                    self.data.append(property_data)
            
            self.respectful_delay()
            return True
            
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Aqar Ù„Ù€ {city}-{property_type}: {e}")
            return False
    
    def scrape_bayut(self, city, property_type):
        """Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Bayut.sa"""
        try:
            city_map = {
                'Ø§Ù„Ø±ÙŠØ§Ø¶': 'riyadh', 
                'Ø¬Ø¯Ø©': 'jeddah', 
                'Ø§Ù„Ø¯Ù…Ø§Ù…': 'dammam',
                'Ù…ÙƒØ©': 'makkah-al-mukarramah',
                'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©': 'al-madinah-al-munawwarah'
            }
            prop_map = {'Ø´Ù‚Ø©': 'apartments', 'ÙÙŠÙ„Ø§': 'villas', 'Ø£Ø±Ø¶': 'land'}
            
            search_city = city_map.get(city, city.lower())
            search_prop = prop_map.get(property_type, property_type.lower())
            
            url = f"https://www.bayut.sa/en/search/{search_city}/{search_prop}/"
            print(f"ğŸ” Ø¬Ø§Ø±ÙŠ Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Bayut: {city} - {property_type}")
            
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Bayut
            listings = soup.find_all(['article', 'div', 'li'], attrs={
                'class': lambda x: x and any(
                    cls in str(x).lower() for cls in ['property', 'listing', 'card', 'item']
                ) if x else False
            })
            
            for listing in listings[:20]:
                property_data = self.parse_bayut_listing(listing, city, property_type)
                if property_data and self.validate_property_data(property_data):
                    self.data.append(property_data)
            
            self.respectful_delay()
            return True
            
        except Exception as e:
            logging.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Bayut Ù„Ù€ {city}-{property_type}: {e}")
            return False
    
    def parse_aqar_listing(self, listing, city, property_type):
        """ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ„ Ø¹Ù‚Ø§Ø± ÙÙŠ Aqar"""
        try:
            price_text = self.extract_text(listing, ['.price', '.cost', '[class*="price"]'])
            price = self.extract_price(price_text)
            
            area_text = self.extract_text(listing, ['.area', '.size', '[class*="area"]'])
            area = self.extract_area(area_text)
            
            district = self.extract_text(listing, ['.district', '.location', '.region', '.address'])
            title = self.extract_text(listing, ['.title', 'h2', 'h3', 'a'])
            
            if price and area:
                return {
                    'City': city,
                    'District': district or city,
                    'Property_Type': property_type,
                    'Price': price,
                    'Area': area,
                    'Source': 'Aqar.fm',
                    'Date': datetime.now().strftime('%Y-%m-%d'),
                    'Title': title or f"{property_type} ÙÙŠ {city}"
                }
        except Exception as e:
            logging.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù‚Ø§Ø± Aqar: {e}")
        return None
    
    def parse_bayut_listing(self, listing, city, property_type):
        """ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ„ Ø¹Ù‚Ø§Ø± ÙÙŠ Bayut"""
        try:
            price_text = self.extract_text(listing, [
                '[aria-label*="price"]', 
                '.price',
                '[class*="price"]',
                '.c4fc20ba'
            ])
            price = self.extract_price(price_text)
            
            area_text = self.extract_text(listing, [
                '[aria-label*="area"]',
                '.area',
                '[class*="area"]',
                '.c4a22b7e'
            ])
            area = self.extract_area(area_text)
            
            district = self.extract_text(listing, [
                '[aria-label*="location"]',
                '.location',
                '.neighborhood'
            ])
            
            title = self.extract_text(listing, [
                'h2',
                'h3',
                '[aria-label*="title"]',
                '.title'
            ])
            
            if price and area:
                return {
                    'City': city,
                    'District': district or city,
                    'Property_Type': property_type,
                    'Price': price,
                    'Area': area,
                    'Source': 'Bayut.sa',
                    'Date': datetime.now().strftime('%Y-%m-%d'),
                    'Title': title or f"{property_type} ÙÙŠ {city}"
                }
        except Exception as e:
            logging.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù‚Ø§Ø± Bayut: {e}")
        return None
    
    def extract_text(self, element, selectors):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… multiple selectors"""
        for selector in selectors:
            try:
                found = element.select_one(selector)
                if found and found.get_text(strip=True):
                    return found.get_text(strip=True)
            except:
                continue
        return ""
    
    def extract_price(self, price_text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¹Ø± Ù…Ù† Ø§Ù„Ù†Øµ"""
        try:
            if 'Ù…Ù„ÙŠÙˆÙ†' in price_text.lower() or 'million' in price_text.lower():
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù…Ø¹ Ø§Ù„ÙƒØ³ÙˆØ±
                numbers = re.findall(r'[\d.]+', price_text)
                if numbers:
                    price = float(numbers[0].replace(',', ''))
                    if price < 100:  # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ù‚Ù… Ø£Ù‚Ù„ Ù…Ù† 100 ÙÙ‡Ùˆ Ø¨Ø§Ù„Ù…Ù„ÙŠÙˆÙ†
                        return int(price * 1000000)
                    return int(price)
            else:
                cleaned = re.sub(r'[^\d]', '', price_text)
                if cleaned:
                    return int(cleaned)
        except:
            pass
        return None
    
    def extract_area(self, area_text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ù…Ù† Ø§Ù„Ù†Øµ"""
        try:
            numbers = re.findall(r'\d+', area_text)
            return int(numbers[0]) if numbers else None
        except:
            return None
    
    def validate_property_data(self, data):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        return (
            data['Price'] is not None and 
            data['Area'] is not None and
            10000 <= data['Price'] <= 50000000 and
            10 <= data['Area'] <= 5000
        )
    
    def respectful_delay(self):
        """ÙØ§ØµÙ„ Ø²Ù…Ù†ÙŠ Ù…Ø­ØªØ±Ù…"""
        time.sleep(random.uniform(2, 4))
