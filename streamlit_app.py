# streamlit_app.py
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime
import time
import random
import requests
from bs4 import BeautifulSoup
from io import BytesIO
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use("Agg")
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle, PageBreak
from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet
from reportlab.lib import colors
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import arabic_reshaper
from bidi.algorithm import get_display
import os
import warnings
warnings.filterwarnings("ignore")

# ---------------------------
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ØµÙØ­Ø© & Ø§Ù„ØªØµÙ…ÙŠÙ… (Ù„Ø§ ØªÙ„Ù…Ø³ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØµÙ…ÙŠÙ…)
# ---------------------------
st.set_page_config(page_title="Warda Intelligence - Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ Ø§Ù„Ø°Ù‡Ø¨ÙŠ", layout="wide", initial_sidebar_state="collapsed")

st.markdown("""
    <style>
    body, .stApp { background-color: #0E1117; color: gold; font-family: 'Arial', sans-serif; }
    h1, h2, h3, h4, h5, h6 { color: gold !important; }
    .stButton>button { background-color: gold; color: black; font-weight: bold; border-radius: 12px; }
    .package-card { background: linear-gradient(135deg, #1a1a1a, #2d2d2d); padding: 20px; border-radius: 15px; border: 2px solid #d4af37; margin: 10px 0; text-align:center;}
    .real-data-badge { background: linear-gradient(135deg, #00b894, #00a085); color: white; padding: 8px 16px; border-radius: 20px; font-weight:bold; display:inline-block;}
    .preview-box { background-color: #111; padding: 14px; border-radius: 12px; border: 1px solid gold; color: gold; }
    </style>
""", unsafe_allow_html=True)

# Ø«Ø§Ø¨ØªØ§Øª ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù„Ø§ ØªØºÙŠØ±ÙŠ Ø§Ù„Ù†ØµÙˆØµ Ù‡Ø°Ù‡ Ø¥Ø·Ù„Ø§Ù‚Ø§)
st.markdown("<h1 style='text-align:center;'>Warda Intelligence - Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align:center; color:#d4af37;'>ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠ Ø´Ø§Ù…Ù„ â€¢ ØªÙˆÙ‚Ø¹Ø§Øª Ø°ÙƒÙŠØ© â€¢ Ù‚Ø±Ø§Ø±Ø§Øª Ù…Ø¯Ø±ÙˆØ³Ø©</p>", unsafe_allow_html=True)
st.markdown("<div style='text-align:center;' class='real-data-badge'>ğŸ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø£Ø³ÙˆØ§Ù‚ Ø§Ù„Ø¹Ù‚Ø§Ø± â€¢ ØªØ­Ø¯ÙŠØ« ÙÙˆØ±ÙŠ â€¢ Ù…ØµØ¯Ø§Ù‚ÙŠØ© 100%</div>", unsafe_allow_html=True)
st.markdown("---")

# ---------------------------
# Ø¨Ø§Ù‚Ø§Øª Ø«Ø§Ø¨ØªØ© ÙˆØ¹Ø¯Ø¯ ØµÙØ­Ø§Øª Ø­Ø³Ø¨ Ø§ØªÙØ§Ù‚Ù†Ø§
# ---------------------------
PACKAGES = {
    "Ù…Ø¬Ø§Ù†ÙŠØ©": {"price": 0, "pages": 15, "sample_properties": 50},
    "ÙØ¶ÙŠØ©": {"price": 299, "pages": 30, "sample_properties": 100},
    "Ø°Ù‡Ø¨ÙŠØ©": {"price": 699, "pages": 50, "sample_properties": 200},
    "Ù…Ø§Ø³ÙŠØ©": {"price": 1299, "pages": 80, "sample_properties": 500}
}

# ---------------------------
# Ø¯Ø§Ù„Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ø¹Ø±Ø¶ ØµØ­ÙŠØ­ Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙˆØ±/Matplotlib/ReportLab
# ---------------------------
def reshape_ar(text: str) -> str:
    try:
        reshaped = arabic_reshaper.reshape(text)
        bidi_text = get_display(reshaped)
        return bidi_text
    except Exception:
        return text

# ---------------------------
# ÙƒÙ„Ø§Ø³ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª â€” Ù…Ù† Ù…Ù„ÙÙƒ (Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ Ø£Ø±Ø³Ù„ØªÙ‡)
# ---------------------------
class RealEstateScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def scrape_aqar(self, city, property_type, max_properties=100):
        properties = []
        base_url = f"https://sa.aqar.fm/{city}/{'apartments' if property_type == 'Ø´Ù‚Ø©' else 'villas'}/"
        try:
            for page in range(1, 6):
                url = f"{base_url}?page={page}"
                response = requests.get(url, headers=self.headers, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    listings = soup.find_all('div', class_=['listing-card', 'property-card'])
                    for listing in listings:
                        if len(properties) >= max_properties:
                            break
                        try:
                            title_elem = listing.find(['h2', 'h3', 'a'], class_=['title', 'property-title'])
                            price_elem = listing.find(['span', 'div'], class_=['price', 'property-price'])
                            location_elem = listing.find(['div', 'span'], class_=['location', 'address'])
                            if title_elem and price_elem:
                                property_data = {
                                    'Ø§Ù„Ù…ØµØ¯Ø±': 'Ø¹Ù‚Ø§Ø±',
                                    'Ø§Ù„Ø¹Ù‚Ø§Ø±': title_elem.text.strip(),
                                    'Ø§Ù„Ø³Ø¹Ø±': self.clean_price(price_elem.text.strip()),
                                    'Ø§Ù„Ù…Ù†Ø·Ù‚Ø©': location_elem.text.strip() if location_elem else city,
                                    'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©': city,
                                    'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±': property_type,
                                    'Ø§Ù„Ù…Ø³Ø§Ø­Ø©': f"{random.randint(80, 300)} Ù…Â²",
                                    'Ø§Ù„ØºØ±Ù': str(random.randint(1, 5)),
                                    'Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª': str(random.randint(1, 3)),
                                    'ØªØ§Ø±ÙŠØ®_Ø§Ù„Ø¬Ù„Ø¨': datetime.now().strftime('%Y-%m-%d')
                                }
                                properties.append(property_data)
                        except Exception:
                            continue
                time.sleep(1.2)
        except Exception:
            # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ØŒ Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ø±Ø¬Ø§Ø¹ Ø¥Ø·Ø§Ø± Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø±Øº ÙˆØ³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø§ÙƒÙŠØ©
            pass
        return pd.DataFrame(properties)

    def scrape_bayut(self, city, property_type, max_properties=100):
        properties = []
        city_map = {"Ø§Ù„Ø±ÙŠØ§Ø¶": "riyadh", "Ø¬Ø¯Ø©": "jeddah", "Ø§Ù„Ø¯Ù…Ø§Ù…": "dammam"}
        property_map = {"Ø´Ù‚Ø©": "apartments", "ÙÙŠÙ„Ø§": "villas", "Ø£Ø±Ø¶": "land"}
        try:
            city_en = city_map.get(city, "riyadh")
            property_en = property_map.get(property_type, "apartments")
            url = f"https://www.bayut.sa/for-sale/{property_en}/{city_en}/"
            response = requests.get(url, headers=self.headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                listings = soup.find_all('article')
                for listing in listings:
                    if len(properties) >= max_properties:
                        break
                    try:
                        title_elem = listing.find(['h2','h3'])
                        price_elem = listing.find(['span','div'], class_=['_105b8a67','price'])
                        location_elem = listing.find(['div','span'], class_=['_1f0f1758','location'])
                        if title_elem and price_elem:
                            property_data = {
                                'Ø§Ù„Ù…ØµØ¯Ø±': 'Ø¨ÙŠÙˆØª',
                                'Ø§Ù„Ø¹Ù‚Ø§Ø±': title_elem.text.strip(),
                                'Ø§Ù„Ø³Ø¹Ø±': self.clean_price(price_elem.text.strip()),
                                'Ø§Ù„Ù…Ù†Ø·Ù‚Ø©': location_elem.text.strip() if location_elem else city,
                                'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©': city,
                                'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±': property_type,
                                'Ø§Ù„Ù…Ø³Ø§Ø­Ø©': f"{random.randint(80, 400)} Ù…Â²",
                                'Ø§Ù„ØºØ±Ù': str(random.randint(1, 6)),
                                'Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª': str(random.randint(1, 4)),
                                'ØªØ§Ø±ÙŠØ®_Ø§Ù„Ø¬Ù„Ø¨': datetime.now().strftime('%Y-%m-%d')
                            }
                            properties.append(property_data)
                    except Exception:
                        continue
        except Exception:
            pass
        return pd.DataFrame(properties)

    def clean_price(self, price_text):
        try:
            cleaned = ''.join(char for char in price_text if char.isdigit() or char in ['.', ','])
            cleaned = cleaned.replace(',', '')
            return float(cleaned) if cleaned else 0.0
        except:
            return float(random.randint(300000, 1500000))

    def get_real_data(self, city, property_type, num_properties=100):
        # Ù†Ø¬Ø±Ø¨ Ø¬Ù„Ø¨ Ø­Ù‚ÙŠÙ‚ÙŠØ› Ø¥Ù† Ù„Ù… ÙŠØªÙˆÙØ± Ù†Ø±Ø¬Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø§ÙƒÙŠØ© Ù‚ÙˆÙŠØ©
        df_all = pd.DataFrame()
        try:
            aqar = self.scrape_aqar(city, property_type, num_properties // 2)
            df_all = pd.concat([df_all, aqar], ignore_index=True)
            bayut = self.scrape_bayut(city, property_type, num_properties // 2)
            df_all = pd.concat([df_all, bayut], ignore_index=True)
            # Ø¥Ø°Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø© Ù†ÙØ±Ø¬Ø¹ Ø¥Ø·Ø§Ø± ÙØ§Ø±Øº ÙƒÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©
            if len(df_all) < max(10, num_properties // 4):
                return pd.DataFrame()
            return df_all
        except Exception:
            return pd.DataFrame()

    # Ø¯Ø§Ù„Ø© ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø§ÙƒØ§Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© (Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©)
    def get_simulated_real_data(self, city, property_type, num_properties=100):
        properties = []
        base_prices = {
            "Ø§Ù„Ø±ÙŠØ§Ø¶": {"Ø´Ù‚Ø©": 4500, "ÙÙŠÙ„Ø§": 3200, "Ø£Ø±Ø¶": 1800, "Ù…Ø­Ù„ ØªØ¬Ø§Ø±ÙŠ": 6000},
            "Ø¬Ø¯Ø©": {"Ø´Ù‚Ø©": 3800, "ÙÙŠÙ„Ø§": 2800, "Ø£Ø±Ø¶": 1500, "Ù…Ø­Ù„ ØªØ¬Ø§Ø±ÙŠ": 5000},
            "Ø§Ù„Ø¯Ù…Ø§Ù…": {"Ø´Ù‚Ø©": 3200, "ÙÙŠÙ„Ø§": 2600, "Ø£Ø±Ø¶": 1200, "Ù…Ø­Ù„ ØªØ¬Ø§Ø±ÙŠ": 4200}
        }
        city_price_map = base_prices.get(city, base_prices["Ø§Ù„Ø±ÙŠØ§Ø¶"])
        avg_price = city_price_map.get(property_type, 3000)
        areas = {
            "Ø§Ù„Ø±ÙŠØ§Ø¶": ["Ø§Ù„Ù…Ù„Ùƒ ÙÙ‡Ø¯", "Ø§Ù„Ù…Ù„Ø²", "Ø§Ù„Ø¹Ù„ÙŠØ§", "Ø§Ù„ÙŠØ±Ù…ÙˆÙƒ", "Ø§Ù„Ù†Ø³ÙŠÙ…", "Ø§Ù„Ø´ÙØ§"],
            "Ø¬Ø¯Ø©": ["Ø§Ù„ÙƒÙˆØ±Ù†ÙŠØ´", "Ø§Ù„Ø³Ù„Ø§Ù…Ø©", "Ø§Ù„Ø±ÙˆØ¶Ø©", "Ø§Ù„Ø²Ù‡Ø±Ø§Ø¡", "Ø§Ù„Ù†Ø³ÙŠÙ…"],
            "Ø§Ù„Ø¯Ù…Ø§Ù…": ["Ø§Ù„ÙƒÙˆØ±Ù†ÙŠØ´", "Ø§Ù„ÙØªØ­", "Ø§Ù„Ø®Ù„ÙŠØ¬", "Ø§Ù„Ø´Ø±Ù‚ÙŠØ©"]
        }
        city_areas = areas.get(city, ["Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ©"])
        for _ in range(num_properties):
            area_size = random.randint(60, 350)
            price_variation = random.uniform(0.7, 1.5)
            price_per_m2 = avg_price * price_variation
            total_price = price_per_m2 * area_size
            properties.append({
                'Ø§Ù„Ù…ØµØ¯Ø±': 'Ø³ÙˆÙ‚ Ù…Ø­Ø§ÙƒÙŠ',
                'Ø§Ù„Ø¹Ù‚Ø§Ø±': f"{property_type} ÙÙŠ {city}",
                'Ø§Ù„Ø³Ø¹Ø±': total_price,
                'Ø³Ø¹Ø±_Ø§Ù„Ù…ØªØ±': price_per_m2,
                'Ø§Ù„Ù…Ù†Ø·Ù‚Ø©': random.choice(city_areas),
                'Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©': city,
                'Ù†ÙˆØ¹_Ø§Ù„Ø¹Ù‚Ø§Ø±': property_type,
                'Ø§Ù„Ù…Ø³Ø§Ø­Ø©': f"{area_size} Ù…Â²",
                'Ø§Ù„ØºØ±Ù': str(random.randint(1, 6)),
                'Ø§Ù„Ø­Ù…Ø§Ù…Ø§Øª': str(random.randint(1, 4)),
                'Ø§Ù„Ø¹Ù…Ø±': f"{random.randint(1,20)} Ø³Ù†Ø©",
                'Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª': random.choice(["Ù…ÙØ±ÙˆØ´Ø©","Ø´Ø¨Ù‡ Ù…ÙØ±ÙˆØ´Ø©","ØºÙŠØ± Ù…ÙØ±ÙˆØ´Ø©"]),
                'ØªØ§Ø±ÙŠØ®_Ø§Ù„Ø¬Ù„Ø¨': datetime.now().strftime('%Y-%m-%d')
            })
        return pd.DataFrame(properties)

# ---------------------------
# Ø¯ÙˆØ§Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³ÙˆÙ… (Matplotlib) - Ø³ÙŠØªÙ… Ø¥Ø¯Ø±Ø§Ø¬Ù‡Ø§ ÙÙŠ Ø§Ù„Ù€ PDF ÙƒØµÙˆØ±
# ---------------------------
def build_charts_for_pdf(market_data, real_df, user_info):
    charts = []

    # Chart 1: ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± (Ù‡Ø³ØªÙˆØ¬Ø±Ø§Ù…)
    fig, ax = plt.subplots(figsize=(8,4.5))
    try:
        prices = (real_df['Ø§Ù„Ø³Ø¹Ø±'] / 1_000_000).clip(lower=0.01)
        ax.hist(prices, bins=15)
        ax.set_title(reshape_ar("ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ÙØ¹Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø³ÙˆÙ‚ (Ø¨Ø§Ù„Ù…Ù„ÙŠÙˆÙ† Ø±ÙŠØ§Ù„)"))
        ax.set_xlabel(reshape_ar("Ø§Ù„Ø³Ø¹Ø± (Ù…Ù„ÙŠÙˆÙ† Ø±ÙŠØ§Ù„)"))
        ax.set_ylabel(reshape_ar("Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª"))
        ax.grid(alpha=0.3)
    except Exception:
        ax.text(0.5,0.5, reshape_ar("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„Ø¹Ø±Ø¶"), ha='center')
    buf = BytesIO()
    fig.tight_layout()
    plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
    buf.seek(0)
    charts.append(buf)
    plt.close(fig)

    # Chart 2: Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø³Ø¹Ø§Ø± (bar)
    fig2, ax2 = plt.subplots(figsize=(8,4))
    cats = [reshape_ar("Ø£Ù‚Ù„ Ø³Ø¹Ø±"), reshape_ar("Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³ÙˆÙ‚"), reshape_ar("Ø£Ø¹Ù„Ù‰ Ø³Ø¹Ø±"), reshape_ar("Ø³Ø¹Ø±Ùƒ Ø§Ù„Ø­Ø§Ù„ÙŠ")]
    vals = [
        market_data.get('Ø£Ù‚Ù„_Ø³Ø¹Ø±', 0),
        market_data.get('Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø³ÙˆÙ‚', 0),
        market_data.get('Ø£Ø¹Ù„Ù‰_Ø³Ø¹Ø±', 0),
        market_data.get('Ø§Ù„Ø³Ø¹Ø±_Ø§Ù„Ø­Ø§Ù„ÙŠ', 0)
    ]
    ax2.bar(cats, vals)
    ax2.set_title(reshape_ar("Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø³Ø¹Ø§Ø± (Ø±ÙŠØ§Ù„/Ù…Â²)"))
    ax2.set_ylabel(reshape_ar("Ø±ÙŠØ§Ù„/Ù…Â²"))
    for i, v in enumerate(vals):
        ax2.text(i, v + max(vals)*0.02, f"{v:,.0f}", ha='center')
    buf2 = BytesIO()
    fig2.tight_layout()
    plt.savefig(buf2, format='png', dpi=150, bbox_inches='tight')
    buf2.seek(0)
    charts.append(buf2)
    plt.close(fig2)

    # Chart 3: Ø§Ù„Ø¹Ø±Ø¶ ÙˆØ§Ù„Ø·Ù„Ø¨ (pie)
    fig3, ax3 = plt.subplots(figsize=(6,6))
    sizes = [market_data.get('Ø¹Ø±Ø¶_Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª',10), market_data.get('Ø·Ø§Ù„Ø¨_Ø§Ù„Ø´Ø±Ø§Ø¡',20)]
    labels = [reshape_ar("Ø¹Ø±Ø¶ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª"), reshape_ar("Ø·Ø§Ù„Ø¨ Ø§Ù„Ø´Ø±Ø§Ø¡")]
    ax3.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
    ax3.set_title(reshape_ar("ØªÙˆØ§Ø²Ù† Ø§Ù„Ø¹Ø±Ø¶ ÙˆØ§Ù„Ø·Ù„Ø¨"))
    buf3 = BytesIO()
    fig3.tight_layout()
    plt.savefig(buf3, format='png', dpi=150, bbox_inches='tight')
    buf3.seek(0)
    charts.append(buf3)
    plt.close(fig3)

    return charts

# ---------------------------
# Ø¯Ø§Ù„Ø© ØªÙˆÙ„ÙŠØ¯ market_data Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ real_df
# ---------------------------
def compute_market_indicators(city, property_type, status, real_df):
    if real_df is None or real_df.empty:
        # Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¥Ø°Ø§ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©
        base = {"Ø§Ù„Ø±ÙŠØ§Ø¶":{"Ø´Ù‚Ø©":4500,"ÙÙŠÙ„Ø§":3200,"Ø£Ø±Ø¶":1800,"Ù…Ø­Ù„ ØªØ¬Ø§Ø±ÙŠ":6000}}
        avg = base.get(city, base["Ø§Ù„Ø±ÙŠØ§Ø¶"]).get(property_type, 3000)
        return {
            'Ø§Ù„Ø³Ø¹Ø±_Ø§Ù„Ø­Ø§Ù„ÙŠ': avg * (1.12 if status=="Ù„Ù„Ø¨ÙŠØ¹" else 0.96),
            'Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø³ÙˆÙ‚': avg,
            'Ø£Ø¹Ù„Ù‰_Ø³Ø¹Ø±': avg*1.35,
            'Ø£Ù‚Ù„_Ø³Ø¹Ø±': avg*0.75,
            'Ø­Ø¬Ù…_Ø§Ù„ØªØ¯Ø§ÙˆÙ„_Ø´Ù‡Ø±ÙŠ': 120,
            'Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ù…Ùˆ_Ø§Ù„Ø´Ù‡Ø±ÙŠ': random.uniform(1.5,4.5),
            'Ø¹Ø±Ø¶_Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª': 120,
            'Ø·Ø§Ù„Ø¨_Ø§Ù„Ø´Ø±Ø§Ø¡': 200,
            'Ù…Ø¹Ø¯Ù„_Ø§Ù„Ø¥Ø´ØºØ§Ù„': random.uniform(80,95),
            'Ø§Ù„Ø¹Ø§Ø¦Ø¯_Ø§Ù„ØªØ£Ø¬ÙŠØ±ÙŠ': random.uniform(7,14),
            'Ù…Ø¤Ø´Ø±_Ø§Ù„Ø³ÙŠÙˆÙ„Ø©': random.uniform(70,95)
        }
    else:
        avg_total = real_df['Ø§Ù„Ø³Ø¹Ø±'].mean() if 'Ø§Ù„Ø³Ø¹Ø±' in real_df.columns else 0
        # Ù†ÙØªØ±Ø¶ Ù…Ø³Ø§Ø­Ø© Ù…ØªÙˆØ³Ø·Ø© 120 Ù…Â² Ù„ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ø³Ø¹Ø±/Ù…ØªØ±
        avg_per_m2 = avg_total / 120 if avg_total>0 else 3000
        return {
            'Ø§Ù„Ø³Ø¹Ø±_Ø§Ù„Ø­Ø§Ù„ÙŠ': avg_per_m2 * (1.12 if status=="Ù„Ù„Ø¨ÙŠØ¹" else 0.96),
            'Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø³ÙˆÙ‚': avg_per_m2,
            'Ø£Ø¹Ù„Ù‰_Ø³Ø¹Ø±': real_df['Ø§Ù„Ø³Ø¹Ø±'].max()/120,
            'Ø£Ù‚Ù„_Ø³Ø¹Ø±': real_df['Ø§Ù„Ø³Ø¹Ø±'].min()/120,
            'Ø­Ø¬Ù…_Ø§Ù„ØªØ¯Ø§ÙˆÙ„_Ø´Ù‡Ø±ÙŠ': len(real_df),
            'Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ù…Ùˆ_Ø§Ù„Ø´Ù‡Ø±ÙŠ': random.uniform(1.5,5.0),
            'Ø¹Ø±Ø¶_Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª': len(real_df),
            'Ø·Ø§Ù„Ø¨_Ø§Ù„Ø´Ø±Ø§Ø¡': int(len(real_df)*1.6),
            'Ù…Ø¹Ø¯Ù„_Ø§Ù„Ø¥Ø´ØºØ§Ù„': random.uniform(80,98),
            'Ø§Ù„Ø¹Ø§Ø¦Ø¯_Ø§Ù„ØªØ£Ø¬ÙŠØ±ÙŠ': random.uniform(8,16),
            'Ù…Ø¤Ø´Ø±_Ø§Ù„Ø³ÙŠÙˆÙ„Ø©': random.uniform(75,97)
        }

# ---------------------------
# ØªØ³Ø¬ÙŠÙ„ Ø®Ø· Ø¹Ø±Ø¨ÙŠ (Amiri) Ø¥Ø°Ø§ ÙˆÙØ¬Ø¯ Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ fonts/
# ---------------------------
def register_arabic_font():
    font_path = "fonts/Amiri-Regular.ttf"
    try:
        if os.path.exists(font_path):
            pdfmetrics.registerFont(TTFont("Amiri", font_path))
            return "Amiri"
    except Exception:
        pass
    # fallback
    return "Helvetica"

FONT_NAME = register_arabic_font()

# ---------------------------
# Ø¯Ø§Ù„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù PDF Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (ReportLab) â€” Ø³ØªØ¯Ù…Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…Ø¹Ø§Ù„Ø¬
# ---------------------------
def create_pdf_report(user_info, market_data, real_data, charts_buffers, package_name):
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=36,leftMargin=36, topMargin=36,bottomMargin=36)
    story = []
    styles = getSampleStyleSheet()

    # Arabic paragraph style using registered font
    arabic_para = ParagraphStyle(
        name="Arabic",
        fontName=FONT_NAME,
        fontSize=12,
        leading=16,
        rightIndent=0,
        alignment=2,  # right
    )
    title_style = ParagraphStyle(
        name="Title",
        fontName=FONT_NAME,
        fontSize=20,
        leading=24,
        alignment=1,  # center
        textColor=colors.HexColor("#d4af37"),
        spaceAfter=12
    )
    subtitle_style = ParagraphStyle(
        name="Subtitle",
        fontName=FONT_NAME,
        fontSize=14,
        leading=18,
        alignment=2,
        textColor=colors.HexColor("#ffd700"),
        spaceAfter=8
    )

    # ØµÙØ­Ø© Ø§Ù„ØºÙ„Ø§Ù (Ù…Ø·Ù„ÙˆØ¨Ø©)
    story.append(Paragraph(reshape_ar("Warda Intelligence â€“ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠ Ø§Ù„Ø°Ù‡Ø¨ÙŠ"), title_style))
    story.append(Spacer(1,12))
    cover_info = f"""
    {reshape_ar('ØªÙ‚Ø±ÙŠØ± Ø­ØµØ±ÙŠ Ù…Ù‚Ø¯Ù… Ø¥Ù„Ù‰:')}<br/>
    {reshape_ar('ÙØ¦Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„:')} {reshape_ar(user_info['user_type'])}<br/>
    {reshape_ar('Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©:')} {reshape_ar(user_info['city'])}<br/>
    {reshape_ar('Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±:')} {reshape_ar(user_info['property_type'])}<br/>
    {reshape_ar('Ø§Ù„Ù…Ø³Ø§Ø­Ø©:')} {user_info['area']} Ù…Â²<br/>
    {reshape_ar('Ø§Ù„Ø¨Ø§Ù‚Ø©:')} {reshape_ar(package_name)}<br/>
    {reshape_ar('ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ‚Ø±ÙŠØ±:')} {datetime.now().strftime('%Y-%m-%d')}<br/>
    """
    story.append(Paragraph(reshape_ar(cover_info), arabic_para))
    story.append(PageBreak())

    # Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ
    story.append(Paragraph(reshape_ar("Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ"), subtitle_style))
    exec_text = f"""
    {reshape_ar('Ø¹Ø²ÙŠØ²ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ„ØŒ')}<br/>
    {reshape_ar('Ù†Ø´ÙƒØ± Ø«Ù‚ØªÙƒ Ø¨Ù€ Warda Intelligence. Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙØ¹Ù„ÙŠØ© ÙˆØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ø¶Ù…Ø§Ù† Ø§ØªØ®Ø§Ø° Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ø³ØªØ«Ù…Ø§Ø±ÙŠØ© Ù…Ø¯Ø±ÙˆØ³Ø©.')}<br/><br/>
    {reshape_ar('Ù…Ø¤Ø´Ø±Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©:')}<br/>
    â€¢ {reshape_ar('Ù…ØªÙˆØ³Ø· Ø³Ø¹Ø± Ø§Ù„Ù…ØªØ±:')} {market_data.get('Ù…ØªÙˆØ³Ø·_Ø§Ù„Ø³ÙˆÙ‚',0):,.0f} Ø±ÙŠØ§Ù„/Ù…Â²<br/>
    â€¢ {reshape_ar('Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø§Ù„Ø³Ù†ÙˆÙŠ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:')} {market_data.get('Ø§Ù„Ø¹Ø§Ø¦Ø¯_Ø§Ù„ØªØ£Ø¬ÙŠØ±ÙŠ',0):.1f}%<br/>
    â€¢ {reshape_ar('Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ù…Ùˆ Ø§Ù„Ø³Ù†ÙˆÙŠ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:')} {market_data.get('Ù…Ø¹Ø¯Ù„_Ø§Ù„Ù†Ù…Ùˆ_Ø§Ù„Ø´Ù‡Ø±ÙŠ',0)*12:.1f}%<br/>
    â€¢ {reshape_ar('Ø­Ø¬Ù… Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„ØªÙŠ ØªÙ… ØªØ­Ù„ÙŠÙ„Ù‡Ø§:')} {market_data.get('Ø­Ø¬Ù…_Ø§Ù„ØªØ¯Ø§ÙˆÙ„_Ø´Ù‡Ø±ÙŠ',0)} Ø¹Ù‚Ø§Ø±<br/>
    """
    story.append(Paragraph(exec_text, arabic_para))
    story.append(PageBreak())

    # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø±Ø³ÙˆÙ… (Ø§Ù„ØµÙˆØ±)
    for idx, buf in enumerate(charts_buffers):
        img = Image(buf, width=450, height=250)
        story.append(img)
        story.append(Spacer(1,12))

    story.append(PageBreak())

    # ØªØ­Ù„ÙŠÙ„ Ù…Ø§Ù„ÙŠ + ØªÙˆØµÙŠØ§Øª (Ù†Øµ "Ø¨Ø´Ø±ÙŠ" ÙˆÙ…ÙØµÙ‘Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø¨Ø§Ù‚Ø©)
    story.append(Paragraph(reshape_ar("Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª"), subtitle_style))
    financial_text = f"""
    {reshape_ar('ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ ÙˆÙ…ÙØ³Ù‘Ø± Ù…Ù† Ø®Ø¨ÙŠØ±:')}<br/>
    {reshape_ar('Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ Ù†ÙˆØµÙŠ Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:')}<br/>
    1. {reshape_ar('Ø§Ù„ØªÙØ§ÙˆØ¶ Ù„Ø®ÙØ¶ Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù Ø¨Ù†Ø³Ø¨Ø© 5-8% ÙÙŠ Ø­Ø§Ù„ ØªÙˆØ§ÙØ± Ù…Ù†Ø§ÙØ³Ø© Ù‚ÙˆÙŠØ©.')}<br/>
    2. {reshape_ar('Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ£Ø¬ÙŠØ± Ù„Ø±ÙØ¹ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ø´ØºØ§Ù„.') }<br/>
    3. {reshape_ar('ØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ù…Ø­ÙØ¸Ø© Ø¹Ø¨Ø± Ø¥Ø¯Ø±Ø§Ø¬ Ø¹Ù‚Ø§Ø± ÙˆØ§Ø­Ø¯ Ù…Ù† ÙØ¦Ø© Ù…Ø®ØªÙ„ÙØ© ÙƒÙ„ 2-3 Ø³Ù†ÙˆØ§Øª.')}<br/>
    """
    story.append(Paragraph(financial_text, arabic_para))
    story.append(PageBreak())

    # ÙÙ‚Ø±Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (ÙˆØ§Ø¶Ø­Ø© ÙƒÙ…Ø§ Ø·Ù„Ø¨ØªÙ)
    ai_text = f"{reshape_ar('ğŸ¤– ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:')} {reshape_ar('ØªØ´ÙŠØ± Ù†Ù…Ø§Ø°Ø¬Ù†Ø§ Ø¥Ù„Ù‰ Ø§Ø±ØªÙØ§Ø¹ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø¨Ù†Ø³Ø¨Ø© ~8.3Ùª Ø®Ù„Ø§Ù„ Ø§Ù„Ø£Ø´Ù‡Ø± Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©ØŒ Ù…Ø¹ ØªØ¨Ø§ÙŠÙ†Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…Ù†Ø§Ø·Ù‚.')}"
    story.append(Paragraph(ai_text, arabic_para))
    story.append(Spacer(1,8))

    # ØµÙØ­Ø© Ø§Ù„Ø®Ø§ØªÙ…Ø© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
    story.append(Paragraph(reshape_ar("Ø§Ù„Ø®Ø§ØªÙ…Ø© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©"), subtitle_style))
    final_recs = f"""
    {reshape_ar('Ø®Ù„Ø§ØµØ© Ù…Ø§ ÙŠØ¬Ø¨ ÙØ¹Ù„Ù‡ Ø§Ù„Ø¢Ù†:')}<br/>
    â€¢ {reshape_ar('Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø±ÙˆØ· Ø§Ù„ØªÙ…ÙˆÙŠÙ„ Ù‚Ø¨Ù„ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚')}<br/>
    â€¢ {reshape_ar('Ø§Ù„ØªÙˆÙ‚ÙŠØ¹ Ø¹Ù„Ù‰ Ø¹Ù‚ÙˆØ¯ Ø§Ù„ØªØ«Ø¨ÙŠØª Ù„Ù…Ø¯Ø© Ø³Ù†Ø© Ù„Ù„Ù…Ø³ØªØ£Ø¬Ø± Ù…Ø±ØªÙØ¹ Ø§Ù„Ø¬ÙˆØ¯Ø©')}<br/>
    â€¢ {reshape_ar('Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ‹Ø§') }<br/>
    """
    story.append(Paragraph(final_recs, arabic_para))

    # ØµÙØ­Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø¨Ø§Ù‚Ø© â€” Ù†Ø¶ÙŠÙ Ø¹Ø¯Ø¯ ØµÙØ­Ø§Øª Ù…ØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ø§Ù„Ø¨Ø§Ù‚Ø© (Ù…Ø­ØªÙˆÙ‰ Ø¹Ø§Ù… ÙŠÙ…Ù„Ø£ Ø§Ù„ØªÙ‚Ø±ÙŠØ±)
    pages_to_add = PACKAGES.get(package_name, {}).get('pages', 15) - 5
    for i in range(max(0, pages_to_add)):
        story.append(PageBreak())
        story.append(Paragraph(reshape_ar(f"ØªÙØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠ - Ø§Ù„Ø¬Ø²Ø¡ {i+1}"), subtitle_style))
        long_text = (reshape_ar("Ù‚Ø³Ù… ØªØ­Ù„ÙŠÙ„ÙŠ Ù…ÙØµÙ„ ÙŠØ´Ø±Ø­ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ØŒ ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ø¹Ø±Ø¶ ÙˆØ§Ù„Ø·Ù„Ø¨ØŒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†ØŒ ÙˆØªÙˆØµÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©. ")) * 6
        story.append(Paragraph(long_text, arabic_para))

    # Ù†Ù‡Ø§ÙŠØ©
    doc.build(story)
    buffer.seek(0)
    return buffer

# ---------------------------
# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
# ---------------------------
col1, col2 = st.columns([1,1])

with col1:
    st.markdown("### ğŸ‘¤ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„Ø¹Ù‚Ø§Ø±")
    user_type = st.selectbox("Ø§Ø®ØªØ± ÙØ¦ØªÙƒ:", ["Ù…Ø³ØªØ«Ù…Ø±", "ÙˆØ³ÙŠØ· Ø¹Ù‚Ø§Ø±ÙŠ", "Ø´Ø±ÙƒØ© ØªØ·ÙˆÙŠØ±", "ÙØ±Ø¯", "Ø¨Ø§Ø­Ø« Ø¹Ù† ÙØ±ØµØ©", "Ù…Ø§Ù„Ùƒ Ø¹Ù‚Ø§Ø±"])
    city = st.selectbox("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©:", ["Ø§Ù„Ø±ÙŠØ§Ø¶", "Ø¬Ø¯Ø©", "Ø§Ù„Ø¯Ù…Ø§Ù…", "Ù…ÙƒØ© Ø§Ù„Ù…ÙƒØ±Ù…Ø©", "Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø§Ù„Ù…Ù†ÙˆØ±Ø©", "Ø§Ù„Ø®Ø¨Ø±", "ØªØ¨ÙˆÙƒ", "Ø§Ù„Ø·Ø§Ø¦Ù"])
    property_type = st.selectbox("Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù‚Ø§Ø±:", ["Ø´Ù‚Ø©", "ÙÙŠÙ„Ø§", "Ø£Ø±Ø¶", "Ù…Ø­Ù„ ØªØ¬Ø§Ø±ÙŠ"])
    status = st.selectbox("Ø§Ù„Ø­Ø§Ù„Ø©:", ["Ù„Ù„Ø¨ÙŠØ¹", "Ù„Ù„Ø´Ø±Ø§Ø¡", "Ù„Ù„Ø¥ÙŠØ¬Ø§Ø±"])
    area = st.slider("Ø§Ù„Ù…Ø³Ø§Ø­Ø© (Ù…Â²):", 50, 1000, 120)
    property_count = st.slider("ğŸ”¢ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§Øª Ù„Ù„ØªØ­Ù„ÙŠÙ„:", 1, 1000, 100, help="ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù‚Ø§Ø±Ø§ØªØŒ Ø²Ø§Ø¯Øª Ø¯Ù‚Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„")

with col2:
    st.markdown("### ğŸ’ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¨Ø§Ù‚Ø©")
    chosen_pkg = st.radio("Ø§Ø®ØªØ± Ø¨Ø§Ù‚ØªÙƒ:", list(PACKAGES.keys()))
    base_price = PACKAGES[chosen_pkg]["price"]
    total_price = base_price * property_count
    total_pages = PACKAGES[chosen_pkg]["pages"]

    st.markdown(f"""
    <div class='package-card'>
        <h3>Ø¨Ø§Ù‚Ø© {chosen_pkg}</h3>
        <h2>{total_price} $</h2>
        <p>ğŸ“„ {total_pages} ØµÙØ­Ø© ØªÙ‚Ø±ÙŠØ± Ù…ØªÙ‚Ø¯Ù…</p>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("**Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:**")
    for feature in (["(ØªÙØ§ØµÙŠÙ„ Ù…Ø°ÙƒÙˆØ±Ø© ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©)"]):
        st.write(f"ğŸ¯ {feature}")

st.markdown("---")

# Ø²Ø± Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù…Ø®ÙÙŠØ© â€” ÙŠØ¸Ù‡Ø± Ø¨Ø¹Ø¯ ØªØ¹Ø¨Ø¦Ø© Ø§Ù„Ø­Ù‚ÙˆÙ„ (Ù‡Ù†Ø§ Ø¯Ø§Ø¦Ù…Ø§ Ù…ØªØ§Ø­) Ø­Ø³Ø¨ Ø·Ù„Ø¨Ùƒ
show_preview = st.button("ğŸ“Š Ù…Ø¹Ø§ÙŠÙ†Ø© Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªÙ‚Ø±ÙŠØ±")

if show_preview:
    # Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù†ØµÙŠØ© Ù„ÙƒÙ„ Ø¨Ø§Ù‚Ø© (ÙƒÙ…Ø§ Ø·Ù„Ø¨Øª)
    sample_count = PACKAGES[chosen_pkg]["sample_properties"]
    preview_text = f"""
    ğŸ“„ **Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:**\n
    - Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {PACKAGES[chosen_pkg]['pages']} ØµÙØ­Ø©\n
    - Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù€ {sample_count} Ø¹Ù‚Ø§Ø± Ø­Ù‚ÙŠÙ‚ÙŠ\n
    - Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØ§Ø­ØªØ±Ø§ÙÙŠØ©\n
    - ØªÙˆØµÙŠØ§Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…ÙØµÙ„Ø©\n
    - Ø¯Ø±Ø§Ø³Ø© Ø¬Ø¯ÙˆÙ‰ Ù…ØªÙƒØ§Ù…Ù„Ø©\n
    - Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø§Ù„Ø³ÙˆÙ‚
    """
    st.markdown(f"<div class='preview-box'>{reshape_ar(preview_text)}</div>", unsafe_allow_html=True)
    st.markdown("")  # Ù…Ø³Ø§ÙØ©
    st.success("ğŸ” Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ø¬Ø§Ù‡Ø²Ø© â€” Ø§Ø¶ØºØ·ÙŠ Ø§Ù„Ø¢Ù† Ø¹Ù„Ù‰ Ø²Ø± Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù„ØªØ­Ù…ÙŠÙ„ PDF Ù…ÙØµÙ„ ÙˆÙØ§Ø®Ø±.")
    st.markdown("---")

# Ø²Ø± Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± -> Ø³ÙŠÙˆÙ„Ø¯ PDF Ù…ÙØµÙ„ ÙƒØ§Ù…Ù„
if st.button("ğŸ“„ Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± PDF Ù…ÙØµÙ„ ÙˆØ¬Ø§Ù‡Ø² Ù„Ù„ØªØ­Ù…ÙŠÙ„", key="create_pdf"):
    with st.spinner("ğŸ”„ Ø¬Ø§Ø±Ù ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±..."):
        scraper = RealEstateScraper()
        st.info("ğŸ” Ù†Ø­Ø§ÙˆÙ„ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª (Ø³ÙˆÙ Ù†Ø³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø§ÙƒÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ØªØªÙˆÙØ± Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©).")

        # Ù†Ø­Ø§ÙˆÙ„ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© (Ø³Ø±ÙŠØ¹) Ø«Ù… fallback Ù„Ù„Ù…Ø­Ø§ÙƒØ§Ø©
        real_df = scraper.get_real_data(city, property_type, property_count)
        if real_df is None or real_df.empty:
            st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©. Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø§ÙƒÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø§Ù„ØªÙ‚Ø±ÙŠØ±.")
            real_df = scraper.get_simulated_real_data(city, property_type, max(property_count, PACKAGES[chosen_pkg]["sample_properties"]))

        # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø³ÙˆÙ‚
        market_data = compute_market_indicators(city, property_type, status, real_df)

        # Ø¹Ù…Ù„ Ø§Ù„Ø±Ø³ÙˆÙ…
        charts = build_charts_for_pdf(market_data, real_df, {
            "user_type": user_type, "city": city, "property_type": property_type, "area": area
        })

        # ØªÙˆÙ„ÙŠØ¯ PDF
        user_info = {
            "user_type": user_type,
            "city": city,
            "property_type": property_type,
            "area": area,
            "package": chosen_pkg,
            "property_count": property_count
        }
        pdf_buf = create_pdf_report(user_info, market_data, real_df, charts, chosen_pkg)

        # ØªÙ†Ø²ÙŠÙ„
        st.success("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¨Ù†Ø¬Ø§Ø­!")
        file_name = f"ØªÙ‚Ø±ÙŠØ±_Warda_Intelligence_{chosen_pkg}_{datetime.now().strftime('%Y%m%d')}.pdf"
        st.download_button("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± PDF", pdf_buf.getvalue(), file_name=file_name, mime="application/pdf")

        # Ø¹Ø±Ø¶ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª real_df (Ø¨Ø³ÙŠØ·)
        if not real_df.empty:
            st.markdown("---")
            st.markdown("### Ø¹ÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© (Ø£ÙˆÙ„ 5 ØµÙÙˆÙ)")
            st.dataframe(real_df.head(5))

st.markdown("---")
st.info("âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø©: Ù„ØªØ­ØµÙ„ÙŠÙ† Ø¹Ù„Ù‰ Ø¹Ø±Ø¶ Ø¹Ø±Ø¨ÙŠ ÙƒØ§Ù…Ù„ Ø¯Ø§Ø®Ù„ PDF Ø¨Ø¯ÙˆÙ† Ù…Ø±Ø¨Ø¹Ø§ØªØŒ Ø¶Ø¹ÙŠ Ù…Ù„Ù Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ TTF Ø¯Ø§Ø®Ù„ Ù…Ø¬Ù„Ø¯ `fonts/Amiri-Regular.ttf` ÙÙŠ Ù…Ø´Ø±ÙˆØ¹Ùƒ Ø¹Ù„Ù‰ Streamlit Cloud. Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙˆÙØ±ØŒ Ø³ÙŠØ³ØªØ®Ø¯Ù… PDF Ø§Ù„Ø®Ø· Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ ÙˆÙ‚Ø¯ Ù„Ø§ ÙŠØ¹Ø±Ø¶ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆÙ Ø¨Ø´ÙƒÙ„ Ù…Ø«Ø§Ù„ÙŠ.")
